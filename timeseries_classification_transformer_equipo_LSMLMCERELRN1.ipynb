{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreaBravo52/RedesNeuronales/blob/main/timeseries_classification_transformer_equipo_LSMLMCERELRN1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea Transformers"
      ],
      "metadata": {
        "id": "friFSnsk7FaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mkdir jena_climate\n",
        "cd jena_climate\n",
        "wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "unzip jena_climate_2009_2016.csv.zip"
      ],
      "metadata": {
        "id": "GRt996gk8JXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "7SzB-l3g8pmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/jena_climate'\n",
        "fname = os.path.join(data_dir, \"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGw3XPd-8hwr",
        "outputId": "1806fdd3-84eb-48d7-eba2-49301d94d890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"Date Time\"', '\"p (mbar)\"', '\"T (degC)\"', '\"Tpot (K)\"', '\"Tdew (degC)\"', '\"rh (%)\"', '\"VPmax (mbar)\"', '\"VPact (mbar)\"', '\"VPdef (mbar)\"', '\"sh (g/kg)\"', '\"H2OC (mmol/mol)\"', '\"rho (g/m**3)\"', '\"wv (m/s)\"', '\"max. wv (m/s)\"', '\"wd (deg)\"']\n",
            "420451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ],
      "metadata": {
        "id": "_Xw8JQX-8uAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_supervised_dataset(data, window_size, n_pred):\n",
        "  X, y = [], []\n",
        "  for i in range(len(data) - window_size - n_pred + 1):\n",
        "      X.append(data[i:i+window_size])\n",
        "      y.append(data[i+window_size:i+window_size+n_pred])\n",
        "\n",
        "  X = np.array(X)\n",
        "  y = np.array(y)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "n_pred=1\n",
        "X, y = create_supervised_dataset(temperature, len(temperature)%256,n_pred)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "1aJRSiL954aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpOxgyeH7jC3"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "Our model processes a tensor of shape `(batch size, sequence length, features)`,\n",
        "where `sequence length` is the number of time steps and `features` is each input\n",
        "timeseries.\n",
        "\n",
        "You can replace your classification RNN layers with this one: the\n",
        "inputs are fully compatible!\n",
        "\n",
        "We include residual connections, layer normalization, and dropout.\n",
        "The resulting layer can be stacked multiple times.\n",
        "\n",
        "The projection layers are implemented through `keras.layers.Conv1D`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMgRvBw67jC3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Attention and Normalization\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(inputs, inputs)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    return x + res\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIKNVuT27jC4"
      },
      "source": [
        "The main part of our model is now complete. We can stack multiple of those\n",
        "`transformer_encoder` blocks and we can also proceed to add the final\n",
        "Multi-Layer Perceptron classification head. Apart from a stack of `Dense`\n",
        "layers, we need to reduce the output tensor of the `TransformerEncoder` part of\n",
        "our model down to a vector of features for each data point in the current\n",
        "batch. A common way to achieve this is to use a pooling layer. For\n",
        "this example, a `GlobalAveragePooling1D` layer is sufficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L9YtHbe7jC4"
      },
      "outputs": [],
      "source": [
        "def build_model(\n",
        "    input_shape,\n",
        "    head_size,\n",
        "    num_heads,\n",
        "    ff_dim,\n",
        "    num_transformer_blocks,\n",
        "    mlp_units,\n",
        "    dropout=0,\n",
        "    mlp_dropout=0,\n",
        "    n_pred=1,\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x) #no se necesita pooling para timeseries de 1 columna\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(n_pred,activation=None)(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAraJfNd7jC5"
      },
      "source": [
        "## Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSIbDeHy7jC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853c4bdc-e0af-443b-f5ef-e1c090b55577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)       [(None, 99, 1)]              0         []                            \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (M  (None, 99, 1)                7169      ['input_11[0][0]',            \n",
            " ultiHeadAttention)                                                  'input_11[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_90 (Dropout)        (None, 99, 1)                0         ['multi_head_attention_40[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_80 (La  (None, 99, 1)                2         ['dropout_90[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_80 (T  (None, 99, 1)                0         ['layer_normalization_80[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'input_11[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_80 (Conv1D)          (None, 99, 4)                8         ['tf.__operators__.add_80[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)        (None, 99, 4)                0         ['conv1d_80[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_81 (Conv1D)          (None, 99, 1)                5         ['dropout_91[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_81 (La  (None, 99, 1)                2         ['conv1d_81[0][0]']           \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_81 (T  (None, 99, 1)                0         ['layer_normalization_81[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_80[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (M  (None, 99, 1)                7169      ['tf.__operators__.add_81[0][0\n",
            " ultiHeadAttention)                                                 ]',                           \n",
            "                                                                     'tf.__operators__.add_81[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)        (None, 99, 1)                0         ['multi_head_attention_41[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_82 (La  (None, 99, 1)                2         ['dropout_92[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_82 (T  (None, 99, 1)                0         ['layer_normalization_82[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_81[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv1d_82 (Conv1D)          (None, 99, 4)                8         ['tf.__operators__.add_82[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)        (None, 99, 4)                0         ['conv1d_82[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_83 (Conv1D)          (None, 99, 1)                5         ['dropout_93[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_83 (La  (None, 99, 1)                2         ['conv1d_83[0][0]']           \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_83 (T  (None, 99, 1)                0         ['layer_normalization_83[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_82[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (M  (None, 99, 1)                7169      ['tf.__operators__.add_83[0][0\n",
            " ultiHeadAttention)                                                 ]',                           \n",
            "                                                                     'tf.__operators__.add_83[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)        (None, 99, 1)                0         ['multi_head_attention_42[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_84 (La  (None, 99, 1)                2         ['dropout_94[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_84 (T  (None, 99, 1)                0         ['layer_normalization_84[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_83[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv1d_84 (Conv1D)          (None, 99, 4)                8         ['tf.__operators__.add_84[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)        (None, 99, 4)                0         ['conv1d_84[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_85 (Conv1D)          (None, 99, 1)                5         ['dropout_95[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_85 (La  (None, 99, 1)                2         ['conv1d_85[0][0]']           \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_85 (T  (None, 99, 1)                0         ['layer_normalization_85[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_84[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (M  (None, 99, 1)                7169      ['tf.__operators__.add_85[0][0\n",
            " ultiHeadAttention)                                                 ]',                           \n",
            "                                                                     'tf.__operators__.add_85[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_96 (Dropout)        (None, 99, 1)                0         ['multi_head_attention_43[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " layer_normalization_86 (La  (None, 99, 1)                2         ['dropout_96[0][0]']          \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_86 (T  (None, 99, 1)                0         ['layer_normalization_86[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_85[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " conv1d_86 (Conv1D)          (None, 99, 4)                8         ['tf.__operators__.add_86[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_97 (Dropout)        (None, 99, 4)                0         ['conv1d_86[0][0]']           \n",
            "                                                                                                  \n",
            " conv1d_87 (Conv1D)          (None, 99, 1)                5         ['dropout_97[0][0]']          \n",
            "                                                                                                  \n",
            " layer_normalization_87 (La  (None, 99, 1)                2         ['conv1d_87[0][0]']           \n",
            " yerNormalization)                                                                                \n",
            "                                                                                                  \n",
            " tf.__operators__.add_87 (T  (None, 99, 1)                0         ['layer_normalization_87[0][0]\n",
            " FOpLambda)                                                         ',                            \n",
            "                                                                     'tf.__operators__.add_86[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " global_average_pooling1d_1  (None, 1)                    0         ['tf.__operators__.add_87[0][0\n",
            " 0 (GlobalAveragePooling1D)                                         ]']                           \n",
            "                                                                                                  \n",
            " dense_20 (Dense)            (None, 128)                  256       ['global_average_pooling1d_10[\n",
            "                                                                    0][0]']                       \n",
            "                                                                                                  \n",
            " dropout_98 (Dropout)        (None, 128)                  0         ['dense_20[0][0]']            \n",
            "                                                                                                  \n",
            " dense_21 (Dense)            (None, 1)                    129       ['dropout_98[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 29129 (113.79 KB)\n",
            "Trainable params: 29129 (113.79 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "3521/3521 [==============================] - 213s 57ms/step - loss: 29.7027 - r2_score: 0.5807 - val_loss: 13.6748 - val_r2_score: 0.8068\n",
            "Epoch 2/5\n",
            "3521/3521 [==============================] - 199s 57ms/step - loss: 16.1120 - r2_score: 0.7726 - val_loss: 13.6056 - val_r2_score: 0.8078\n",
            "Epoch 3/5\n",
            "3521/3521 [==============================] - 197s 56ms/step - loss: 15.6675 - r2_score: 0.7788 - val_loss: 13.6138 - val_r2_score: 0.8076\n",
            "Epoch 4/5\n",
            "3521/3521 [==============================] - 197s 56ms/step - loss: 15.5066 - r2_score: 0.7811 - val_loss: 13.5983 - val_r2_score: 0.8079\n",
            "Epoch 5/5\n",
            "3521/3521 [==============================] - 197s 56ms/step - loss: 15.4594 - r2_score: 0.7818 - val_loss: 13.5840 - val_r2_score: 0.8081\n",
            "4335/4335 [==============================] - 49s 11ms/step - loss: 13.7710 - r2_score: 0.8062\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[13.771004676818848, 0.80619215965271]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X_train_dim = tf.expand_dims(X_train,-1)\n",
        "input_shape = X_train_dim.shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=4,\n",
        "    mlp_units=[128],\n",
        "    mlp_dropout=0.4,\n",
        "    dropout=0.25,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    loss=\"mse\",\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    metrics=[keras.metrics.R2Score()]\n",
        ")\n",
        "model.summary()\n",
        "\n",
        "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
        "\n",
        "model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gheosW617jC6"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "Hicimos nada mas 5 epochs ya que tardaba varios minutos en cada una. Aun así, con dicho número se lograron buenos resultados con una R cuadrada de .80 Un nivel bastante aceptable para un primer approach a los transformers."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}